import { NextRequest } from "next/server";
import { createOpenRouter } from "@openrouter/ai-sdk-provider";
import { streamText } from "ai";

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// CONFIGURACIÃ“N â€” Cascade de modelos por costo
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/**
 * Cascade verificado con API real de OpenRouter (Feb 2026).
 * google/gemini-2.0-flash:free NO existe â€” no usar.
 *
 * Orden por costo (free primero, pago al final):
 *
 *  1. openrouter/free              â†’ $0.00  ðŸ¤– Meta-router: elige el mejor free auto (ctx 200K, VISION)
 *  2. qwen/qwen3-vl-30b-a3b-thinking â†’ $0.00  ðŸ‘ï¸  VisiÃ³n + razonamiento (ctx 131K)
 *  3. mistralai/mistral-small-3.1-24b-instruct:free â†’ $0.00  ðŸ‘ï¸  VisiÃ³n (ctx 128K)
 *  4. google/gemini-2.0-flash-lite-001 â†’ pago   ðŸ’° $0.075/$0.30 por M tok (mÃ¡s barato pagado)
 */
const MODEL_CASCADE = [
    "openrouter/free",
    "qwen/qwen3-vl-30b-a3b-thinking",
    "mistralai/mistral-small-3.1-24b-instruct:free",
    "google/gemini-2.0-flash-lite-001",   // fallback pago ~$0.00019/consulta
];

const MAX_HISTORY_MSGS = 6;
const MAX_MSG_CHARS = 600;
const MAX_OUTPUT_TOKENS = 550;

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// PROMPTS compactos
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const VISION_PROMPT = `Sos Cerebro, el sistema tÃ©cnico de MACCELL. AnalizÃ¡ la imagen de placa electrÃ³nica.
MÃ¡x 6 lÃ­neas. Formato:
DAÃ‘O: [componentes quemados/sulfatados/faltantes]
SECTOR: [RF/Carga/Backlight/CPU/PMIC/etc]
DIAGNÃ“STICO: [falla probable + mediciÃ³n sugerida]
ACCIÃ“N: [reballing/reemplazo IC/limpieza/etc]
IDENTIDAD: Nunca menciones que sos Gemini, Llama, GPT ni ningÃºn modelo externo. Sos Cerebro.
Si no es placa electrÃ³nica, pedÃ­ mejor imagen.`;

const SYSTEM_PROMPT = `Eres "Cerebro", el nÃºcleo de inteligencia tÃ©cnica de MACCELL (San Luis, Argentina). Sistema propietario de diagnÃ³stico electrÃ³nico para reparaciÃ³n de celulares y microsoldadura.

IDENTIDAD â€” MUY IMPORTANTE:
- Si te preguntan quÃ© IA sos, modelo, quien te creÃ³: respondÃ© SOLO "Soy Cerebro, el sistema de diagnÃ³stico tÃ©cnico de MACCELL." Nunca menciones Gemini, Google, Llama, Meta, GPT, OpenAI ni ningÃºn proveedor externo.
- Si preguntan por "la base de datos": tenÃ©s acceso a miles de casos de reparaciÃ³n de MACCELL. RespondÃ© en base a tu entrenamiento tÃ©cnico.

COMPORTAMIENTO:
- Sin saludos. Directo al diagnÃ³stico.
- IdentificÃ¡ ICs por nombre (PMIC, Tristar, U2, Hydra, etc).
- Respuestas cortas y estructuradas.
- El prefijo [TÃ©cnico Nombre]: en los mensajes es solo identificaciÃ³n del tÃ©cnico, ignoralo para el diagnÃ³stico.

FORMATO DE RESPUESTA:
### ðŸ” DIAGNÃ“STICO
[anÃ¡lisis de la falla]
### ðŸ› ï¸ MEDICIÃ“N
- [punto] â†’ [valor esperado]
### ðŸŽ¯ ACCIÃ“N
[pasos concretos]

Sin datos de consumo â†’ pedÃ­ la TRIADA: 1)Amperaje fuente (encendido/apagado) 2)TensiÃ³n VBUS 3)Reconocimiento USB en PC`;

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// HELPERS
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function hasImageParts(messages: any[]): boolean {
    const last = messages[messages.length - 1];
    if (!last) return false;
    if (Array.isArray(last.parts)) {
        return last.parts.some((p: any) => p.type === 'file' && p.mediaType?.startsWith('image/'));
    }
    if (Array.isArray(last.experimental_attachments)) {
        return last.experimental_attachments.some((a: any) => a.contentType?.startsWith('image/'));
    }
    return false;
}

function truncate(text: string, max = MAX_MSG_CHARS): string {
    return text.length <= max ? text : text.slice(0, max) + 'â€¦';
}

function toCoreMsgs(messages: any[]): any[] {
    const lastMsg = messages[messages.length - 1];
    const history = messages.slice(0, -1).slice(-MAX_HISTORY_MSGS + 1);
    const trimmed = [...history, lastMsg];

    return trimmed
        .filter((m: any) => m.role === 'user' || m.role === 'assistant')
        .map((m: any) => {
            if (Array.isArray(m.parts) && m.parts.length > 0) {
                const contentParts: any[] = [];
                for (const part of m.parts) {
                    if (part.type === 'text') {
                        const text = truncate(part.text || '');
                        if (text.trim()) contentParts.push({ type: 'text', text });
                    } else if (part.type === 'file') {
                        const url = part.url || '';
                        if (url) contentParts.push({ type: 'image', image: url });
                    }
                }
                if (!contentParts.some((p: any) => p.type === 'text') && m.content) {
                    contentParts.unshift({ type: 'text', text: truncate(m.content) });
                }
                return {
                    role: m.role,
                    content: contentParts.length > 0 ? contentParts : truncate(m.content || ''),
                };
            }
            return { role: m.role, content: truncate(m.content || '') };
        })
        .filter((m: any) => m.content && (typeof m.content === 'string' ? m.content.trim() : m.content.length > 0));
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// HANDLER
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export async function POST(req: NextRequest) {
    const openrouterKey = process.env.OPENROUTER_API_KEY;
    if (!openrouterKey) {
        return new Response("Error: OPENROUTER_API_KEY no configurada.", { status: 500 });
    }

    let body: any;
    try { body = await req.json(); } catch {
        return new Response("JSON invÃ¡lido.", { status: 400 });
    }

    const messages: any[] = body.messages || [];
    if (!messages.length) return new Response("No messages.", { status: 400 });

    const visionMode = hasImageParts(messages);
    const systemPrompt = visionMode ? VISION_PROMPT : SYSTEM_PROMPT;
    const coreMessages = toCoreMsgs(messages);
    if (coreMessages.length === 0) return new Response("No valid messages.", { status: 400 });

    const openrouter = createOpenRouter({ apiKey: openrouterKey });

    // Cascade directo sin ping previo â€” evita 8-10 seg de overhead por consulta.
    // streamText lanza sincrÃ³nicamente si el modelo rechaza el request (401/429/503).
    // En ese caso, pasamos al siguiente modelo del cascade.
    for (let i = 0; i < MODEL_CASCADE.length; i++) {
        const modelId = MODEL_CASCADE[i];
        const isFree = modelId.endsWith(':free') || modelId === 'openrouter/free';
        const isLast = i === MODEL_CASCADE.length - 1;

        try {
            const result = streamText({
                model: openrouter(modelId),
                system: systemPrompt,
                messages: coreMessages,
                temperature: 0.3,
                maxOutputTokens: MAX_OUTPUT_TOKENS,
            });

            console.log(`[CEREBRO] â–¶ ${modelId} (${isFree ? 'GRATIS ðŸŽ‰' : 'pago ~$0.00019'}) | ${visionMode ? 'VISIÃ“N' : 'TEXTO'}`);

            return result.toUIMessageStreamResponse({
                headers: {
                    'Cache-Control': 'no-cache, no-store',
                    'X-Accel-Buffering': 'no',
                    'X-Model-Used': modelId,
                    'X-Model-Tier': isFree ? 'free' : 'paid',
                }
            });

        } catch (error: any) {
            const status = error?.status ?? error?.statusCode;
            const msg = String(error?.message || '');

            if (status === 401 || msg.includes('User not found')) {
                return new Response("âŒ API Key de OpenRouter invÃ¡lida.", { status: 401 });
            }

            const isRetryable = status === 429 || status === 503 || status === 502
                || msg.includes('rate limit') || msg.includes('overloaded')
                || msg.includes('unavailable') || msg.includes('quota');

            if ((isRetryable || isFree) && !isLast) {
                console.warn(`[CEREBRO] âš ï¸ ${modelId} fallÃ³ (${status ?? msg.slice(0, 60)}). Probando siguiente...`);
                continue;
            }

            console.error(`[CEREBRO] âŒ Error con ${modelId}:`, error);
            return new Response(`âŒ Error: ${error.message || 'Error desconocido'}`, { status: 500 });
        }
    }

    return new Response("âŒ Sin modelos disponibles. IntentÃ¡ en unos minutos.", { status: 503 });
}

