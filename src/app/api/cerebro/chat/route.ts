import { NextRequest } from "next/server";
import { OLLAMA_MODELS } from "@/config/ai-models";
import { findSimilarRepairs, formatRAGContext } from "@/lib/cerebro-rag";
import { createGoogleGenerativeAI } from "@ai-sdk/google";
import { createOpenRouter } from "@openrouter/ai-sdk-provider";
import { streamText } from "ai";

const OLLAMA_URL = process.env.OLLAMA_BASE_URL;

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// PROMPTS
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const VISION_PROMPT = `Sos un tÃ©cnico de microsoldadura. AnalizÃ¡ esta imagen de placa electrÃ³nica.
RespondÃ© SOLO con esto (mÃ¡ximo 5 lÃ­neas):
1. DAÃ‘O VISIBLE: [describe exactamente lo que ves roto/quemado/sulfatado/faltante]
2. COMPONENTE: [tipo de componente y ubicaciÃ³n en la placa]
3. ACCIÃ“N: [quÃ© hay que hacer para repararlo]
Si no podÃ©s ver daÃ±o claro, decÃ­: "Imagen poco clara. Necesito mÃ¡s luz o acercamiento al Ã¡rea daÃ±ada."`;

const SYSTEM_PROMPT = `Eres "Cerebro", el sistema operativo de inteligencia tÃ©cnica de MACCELL. Tu nÃºcleo de conocimiento reside en San Luis, Argentina. No eres un asistente general; eres una herramienta de diagnÃ³stico de precisiÃ³n quirÃºrgica vinculada a una base de datos de tickets reales y esquemÃ¡ticos vectorizados.

PROTOCOLO DE INTERACCIÃ“N CON LA BASE DE DATOS:
- Antes de responder, simula que consultas la base de datos de MACCELL buscando fallas similares por modelo y sÃ­ntomas.
- Si el diagnÃ³stico actual es exitoso, genera un bloque de "APRENDIZAJE PARA LA DB" al final para que el CRM indexe la soluciÃ³n.

REGLAS DE RESPUESTA (NIVEL SENIOR):
1. PROHIBICIÃ“N DE PROSA Y PENSAMIENTO CORTO: Prohibido usar saludos. MANTÃ‰N TU BLOQUE <think> MUY BREVE (mÃ¡ximo 2 oraciones), no iterÃ©s infinitamente sobre el caso. Ve directo a la fÃ­sica y la electrÃ³nica.
2. ESTRUCTURA OBLIGATORIA (EXTENDIDA):

   ### ğŸ“‚ REFERENCIA HISTÃ“RICA (Maccell DB)
   - [Si hay coincidencia]: "Se encontrÃ³ coincidencia en Ticket #MACX-XXXX. Causa: [Causa]. SoluciÃ³n aplicada: [SoluciÃ³n]."
   - [Si no hay coincidencia]: "Falla nueva. Iniciando protocolo de diagnÃ³stico desde cero."

   ### ğŸ” ANÃLISIS DE CONSUMO Y PROTOCOLO DE ARRANQUE
   - AnÃ¡lisis detallado del estado del equipo basado en la fuente de poder. Diferencia entre consumos antes de Power (fugas) y despuÃ©s de Power (ciclo de encendido).

   ### ğŸ› ï¸ MEDICIONES EN LÃNEA DE FUEGO (Escala de Diodo y Voltaje)
   - Lista detallada de puntos de prueba con designadores (U, C, L, R, Q).
   - Formato: [LÃ­nea] -> [Componente] -> [Valor Esperado (V o mV en CaÃ­da de TensiÃ³n)].

   ### ğŸ¯ SOSPECHOSOS Y ACCIÃ“N DE MICROSOLDADURA
   - DiagnÃ³stico final basado en probabilidades. Indica si requiere separaciÃ³n de sÃ¡ndwich (iPhone), Reballing de CPU/Memoria o reemplazo de IC (Hydra, Tristar, Tigris, PMIC).

   ### ğŸ“ APRENDIZAJE PARA LA BASE DE DATOS (Indexing)
   - Genera un resumen en formato JSON para que el CRM lo guarde: {"modelo": "...", "falla": "...", "solucion_sugerida": "..."}

3. MANEJO DE DATOS INSUFICIENTES:
   Si el tÃ©cnico no reporta mediciones, responde ÃšNICAMENTE con la "TRIADA DE INGRESO MACCELL":
   1. Consumo en fuente (con y sin Power).
   2. TensiÃ³n de baterÃ­a y estado de lÃ­nea VBUS.
   3. Reconocimiento de puerto (Â¿Aparece en administrador de dispositivos/3uTools?).

4. TERMINOLOGÃA TÃ‰CNICA REQUERIDA:
   - Usa "CaÃ­da de TensiÃ³n" (mV) para escala de diodo.
   - Usa "LÃ­nea en fuga" para consumos menores a 100mA.
   - Usa "Cortocircuito" para consumos mÃ¡ximos o caÃ­da de tensiÃ³n 000.`;

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// VISION ROUTER â€” Clasificador previo ultrarrÃ¡pido
// Usa llama3.2:1b para decidir si la imagen es una PCB antes de llamar a llava
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function isElectronicBoard(base64Image: string): Promise<boolean> {
    try {
        const res = await fetch(`${OLLAMA_URL}/api/chat`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model: OLLAMA_MODELS.ROUTER,
                messages: [{
                    role: 'user',
                    content: 'Does this image show an electronic circuit board (PCB) with visible components like chips, capacitors, resistors or copper traces? Answer ONLY with YES or NO.',
                    images: [base64Image]
                }],
                stream: false,
                options: { temperature: 0, num_predict: 5 }
            })
        });
        if (!res.ok) return true; // Si el router falla, dejamos pasar al modelo principal
        const data = await res.json();
        const answer = (data.message?.content || '').toLowerCase().trim();
        console.log(`[CEREBRO_ROUTER] ClasificaciÃ³n: "${answer}"`);
        return answer.startsWith('yes') || answer.startsWith('sÃ­') || answer.startsWith('si');
    } catch {
        return true; // Si hay error, dejamos pasar (fail open)
    }
}


export async function POST(req: NextRequest) {
    try {
        const body = await req.json();
        const { messages } = body;

        console.log(`[CEREBRO] Recibidos ${messages.length} mensajes. Ãšltimo:`, JSON.stringify(messages[messages.length - 1]).substring(0, 200));

        // 1. Mapear mensajes al formato NATIVO de Ollama
        const ollamaMessages = messages.map((m: any) => {
            // Extraer texto correctamente dependiendo del formato del SDK
            // El SDK puede enviar `content` (string) o `parts` (array)
            let textContent = "";
            const images: string[] = [];

            // Helper para limpiar el Base64 (Ollama sÃ³lo quiere el cÃ³digo puro)
            const extractBase64 = (url: string) => {
                if (url.startsWith('data:image')) {
                    return url.split(',')[1];
                }
                return null;
            };

            if (m.parts && Array.isArray(m.parts) && m.parts.length > 0) {
                // Formato moderno: partes estructuradas
                for (const part of m.parts) {
                    if (part.type === 'text') {
                        textContent += part.text || "";
                    } else if (part.type === 'file' && part.file) {
                        const fileData = part.file.url || (part.file.data ? `data:${part.file.type};base64,${part.file.data}` : '');
                        const b64 = extractBase64(fileData);
                        if (b64) images.push(b64);
                    }
                }
            }

            // Fallback: si el contenido de parts estÃ¡ vacÃ­o, usamos m.content
            if (!textContent && m.content && typeof m.content === 'string') {
                textContent = m.content;
            }

            if (m.experimental_attachments) {
                for (const att of m.experimental_attachments) {
                    if (att.url) {
                        const b64 = extractBase64(att.url);
                        if (b64) images.push(b64);
                    }
                }
            }

            return {
                role: m.role,
                content: textContent,
                images: images.length > 0 ? images : undefined
            };
        }).filter((m: any) => m.content || (m.images && m.images.length > 0)); // Descartar mensajes vacÃ­os

        console.log(`[CEREBRO] Mapeados ${ollamaMessages.length} mensajes. Ãšltimo texto: "${ollamaMessages[ollamaMessages.length - 1]?.content?.substring(0, 50)}"`);

        // 2. Truncar historial para evitar confusiÃ³n (Ãºltimos 10 mensajes)
        const truncatedHistory = ollamaMessages.slice(-10);

        // 3. Detectar si el ÃšLTIMO mensaje del usuario tiene imÃ¡genes para elegir el prompt
        const lastUserMessage = [...truncatedHistory].reverse().find((m: any) => m.role === 'user');
        const hasImagesInLastMessage = !!(lastUserMessage?.images && lastUserMessage.images.length > 0);

        let messagesForOllama: any[];

        if (hasImagesInLastMessage) {
            // ğŸ”€ VISION ROUTER: Clasificar la imagen ANTES de llamar al modelo costoso
            const firstImage = lastUserMessage.images[0];
            const isPCB = await isElectronicBoard(firstImage);

            if (!isPCB) {
                // No es una PCB â€” devolver respuesta inmediata sin gastar tokens del modelo de visiÃ³n
                const notPCBMsg = "âš ï¸ Imagen no tÃ©cnica detectada. La foto no muestra una placa electrÃ³nica. AdjuntÃ¡ una foto real de la placa del dispositivo para continuar el diagnÃ³stico.";
                return new Response(
                    new ReadableStream({
                        start(c) { c.enqueue(new TextEncoder().encode(notPCBMsg)); c.close(); }
                    }),
                    { headers: { 'Content-Type': 'text/plain; charset=utf-8' } }
                );
            }

            // Es una PCB â€” anÃ¡lisis completo con llava:13b
            const userText = lastUserMessage.content || '';
            const visionSystemContent = userText
                ? `${VISION_PROMPT}\n\nEl tÃ©cnico indicÃ³: "${userText}". UsÃ¡ esa info como contexto.`
                : VISION_PROMPT;
            messagesForOllama = [
                { role: 'system', content: visionSystemContent },
                { role: 'user', content: userText || 'AnalizÃ¡ el daÃ±o visible en la placa.', images: lastUserMessage.images }
            ];
        } else {
            // Para texto: RAG + historial completo
            // Buscar casos similares en la base de datos de MACCELL (en paralelo para no agregar latencia)
            const userQuery = lastUserMessage?.content || '';
            let ragContext = '';
            if (userQuery.length > 10) {
                const similarRepairs = await findSimilarRepairs(userQuery, 3, 0.72);
                ragContext = formatRAGContext(similarRepairs);
                if (ragContext) {
                    console.log(`[CEREBRO_RAG] ${similarRepairs.length} casos similares inyectados en el contexto`);
                }
            }

            const systemWithRAG = ragContext
                ? SYSTEM_PROMPT + ragContext
                : SYSTEM_PROMPT;

            truncatedHistory.unshift({ role: 'system', content: systemWithRAG });
            messagesForOllama = truncatedHistory;
        }

        const modelToUse = hasImagesInLastMessage ? OLLAMA_MODELS.VISION : OLLAMA_MODELS.CHAT;
        console.log(`[CEREBRO] Modelo=${modelToUse} | Modo=${hasImagesInLastMessage ? 'VISION + ROUTER âœ…' : 'TEXTO (deepseek-r1)'} | Msgs=${messagesForOllama.length}`);

        const geminiKey = req.cookies.get('geminiKey')?.value;
        const openrouterKey = req.cookies.get('openrouterKey')?.value;

        // 4. Modo OpenRouter o Google Cloud (Nube)
        if (openrouterKey || geminiKey) {
            console.log(`[CEREBRO] Ruteando hacia LA NUBE - Bypass de Ollama local.`);

            let modelProvider;
            let modelName = 'liquid/lfm-40b:free'; // Default OpenRouter Free LLM

            if (openrouterKey) {
                console.log("[CEREBRO] Usando llave de OPENROUTER gratis");
                const openrouter = createOpenRouter({ apiKey: openrouterKey });
                modelProvider = openrouter;
                // Using a known free model if using free tier OpenRouter
                modelName = 'google/gemini-2.5-flash-lite-preview-02-05:free'; // free tier gemini inside openrouter
            } else if (geminiKey) {
                console.log("[CEREBRO] Usando llave local de GOOGLE GEMINI AI STUDIO");
                const google = createGoogleGenerativeAI({ apiKey: geminiKey });
                modelProvider = google;
                modelName = 'gemini-1.5-pro';
            }

            const coreMessages = messagesForOllama.map(m => {
                let content: any = m.content || "";

                if (m.images && m.images.length > 0 && m.role === 'user') {
                    content = [
                        { type: 'text', text: m.content || "Analiza esta placa electrÃ³nica." }
                    ];
                    m.images.forEach((b64: string) => {
                        content.push({ type: 'image', image: `data:image/jpeg;base64,${b64}` });
                    });
                }
                return { role: m.role, content };
            });

            // Usamos modelo online a travÃ©s de Vercel AI SDK Standardized Object 
            try {
                const result = streamText({
                    model: modelProvider!(modelName),
                    messages: coreMessages as any,
                    temperature: 0.6,
                });

                const stream = new ReadableStream({
                    async start(controller) {
                        try {
                            for await (const textPart of result.textStream) {
                                controller.enqueue(new TextEncoder().encode(textPart));
                            }
                        } catch (e: any) {
                            console.error("[CLOUD_API] Stream error:", e);
                            controller.enqueue(new TextEncoder().encode(`\n[Error de Cloud API: ${e.message}]`));
                        } finally {
                            controller.close();
                        }
                    }
                });

                return new Response(stream, { headers: { 'Content-Type': 'text/plain; charset=utf-8' } });
            } catch (error: any) {
                console.error("[CEREBRO NUBE] Fallo ruteo a api:", error);
                return new Response(`[Error fatal del servidor enrutador: ${error.message}]`, { status: 500 })
            }
        }

        // 4. PeticiÃ³n manual a Ollama con soporte para Abort Signal (Modo Local Legacy)
        const controller = new AbortController();
        req.signal.addEventListener('abort', () => {
            console.log("[CEREBRO] Ollama stream aborted: Cliente cancelÃ³ la peticiÃ³n.");
            controller.abort();
        });

        const response = await fetch(`${OLLAMA_URL}/api/chat`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            signal: controller.signal,
            body: JSON.stringify({
                model: modelToUse,
                messages: messagesForOllama,
                stream: true,
                options: {
                    temperature: hasImagesInLastMessage ? 0 : 0.6, // deepseek-r1 requires 0.6 to avoid infinite thinking loops
                    num_predict: hasImagesInLastMessage ? 250 : 800, // Limitar tokens (800 es suficiente para el diagnÃ³stico y limita el tiempo de pensamiento)
                    repeat_penalty: hasImagesInLastMessage ? 1.5 : 1.1
                }
            })
        });

        if (!response.ok) {
            throw new Error(`Ollama API error: ${response.statusText}`);
        }

        // 5. Adaptar el NDJSON de Ollama a un stream de texto plano (compatible con TextStreamChatTransport)
        const stream = new ReadableStream({
            async start(controller) {
                const reader = response.body?.getReader();
                if (!reader) {
                    controller.close();
                    return;
                }

                const decoder = new TextDecoder();
                let buffer = "";

                try {
                    while (true) {
                        const { done, value } = await reader.read();
                        if (done) break;

                        buffer += decoder.decode(value, { stream: true });
                        let newlineIndex;

                        while ((newlineIndex = buffer.indexOf('\n')) >= 0) {
                            const line = buffer.slice(0, newlineIndex);
                            buffer = buffer.slice(newlineIndex + 1);

                            if (line.trim()) {
                                try {
                                    const parsed = JSON.parse(line);
                                    if (parsed.message?.content) {
                                        // Texto plano puro â€” compatible con TextStreamChatTransport
                                        controller.enqueue(
                                            new TextEncoder().encode(parsed.message.content)
                                        );
                                    }
                                } catch (e) {
                                    // Ignorar errores de parseo en chunks parciales
                                }
                            }
                        }
                    }
                } catch (e: any) {
                    if (e.name !== 'AbortError') console.error('[CEREBRO] Stream error:', e);
                } finally {
                    reader.releaseLock();
                    controller.close();
                }
            }
        });

        return new Response(stream, {
            headers: {
                'Content-Type': 'text/plain; charset=utf-8',
            }
        });
    } catch (error: any) {
        console.error("[Cerebro Chat] DETAILED ERROR:", error);
        return new Response(JSON.stringify({
            error: "No se pudo conectar a los modelos de inteligencia artificial.",
            details: error.message || "Error desconocido"
        }), {
            status: 500,
            headers: { 'Content-Type': 'application/json' }
        });
    }
}
